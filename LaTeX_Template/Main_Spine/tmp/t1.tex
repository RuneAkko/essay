草稿：native gcc

基于单向队列时延梯度的码率控制机制

第一部分 到达时间模型

概况

使用单向队列时延梯度作为拥塞控制的信号量，可以在不引入过大队列缓存的情况下，进行拥塞控制和带宽估计。基于单向队列时延梯度的拥塞控制机制，要求高精度的时延测量与估计。单向队列时延梯度，也就是极小间隔内的单向队列时延变化率。但队列时延是无法直接测量的。数据的发送和到达时间差，表征了总的传输时延$D_{total}$，它一般包括传播时延$D_{trans}$、队列时延$D_{q}$和网络抖动时延$D_{j}$：

\begin{equation*}
D_{total} = D_{trans} + D_{q} + D_{j}
\end{equation*}

设第$i$、$i+1$步的rtp包的发送时间戳和到达时间戳分别为：$T_{departure}(i)$,$T_{arrival}(i)$和$T_{departure}(i+1)$,$T_{arrival}(i+1)$，显然相邻时间步的$D_{trans}$相同，则有:
\begin{align*}
D_{total}(i+1)-D_{total}(i)
& = D_{trans}(i+1) + D_{q}(i+1) + D_{j}(i+1) - D_{trans}(i)  - D_{q}(i) - D_{j}(i) \\
& = \Delta{D_{q}} +\Delta{D_{j}}
\end{align*}

可以认为$D_{total}(i+1)-D_{total}(i)$是$\Delta{D_{q}}$的测量值，记为$M_{D}$,$\Delta{D_{j}}$是系统噪声。为了最大限度排除其他时延噪声干扰，得到精确的队列时延变化率，可以进行系统建模，使用滤波器来计算$\Delta{D_{q}}$的最佳估计值$\hat{\Delta{D_{q}}}$。

原始算法的滤波器使用有卡尔曼滤波和最小二乘法线性回归估计，为了更精确地进行滤波，本文使用拓展卡尔曼滤波，因为实际系统更接近时变非线性系统。


预过滤器：划分包组

因为流媒体协议RTP/RTCP的传输层是udp协议，因而WebRTC在发送包时，使用平滑发包机制(pacer)。按草案\parencite{rmcat-gcc}建议，一般将$5ms$内封装好的rtp包同时发出，形成规律的脉冲，这种间隔即burst interval。因而，发送时间在$5ms$内的包，可以看作一个包组，这也是测量时延的最小粒度。

除了平滑发包之外，网络链路的瞬时中断(outage)也会影响rtp包的时延。瞬时中断造成过量rtp包堆积在发送端缓存中，并在后续网络恢复后，同时被发送。显然，这种情况影响下的rtp包也应看作一个包组，因为他们的实际时延是类似的。具体而言，在同时满足
\begin{enumerate}
    \item 当前包与上一个包的到达时间间隔小于burst interval；
    \item 当前包与当前组的最新包(即上一个包)相比，时延减小；
\end{enumerate}
两个条件下，也认为属于同一个包组。

滤波器估计队列时延变化率

卡尔曼滤波器

最小二乘线性回归

在MXX之后的Chrome版本中，WebRTC使用了最小二乘法，即使用最小误差平方和，来对累计时延-时间曲线进行线性回归，将得到的直线方程斜率，作为队列时延变化率。该算法部署在发送端，因而不能直接通过接收到的RTP包的统计情况来计算样本点。由草案规定的[]RTCP feedback报文，来进行采样、线性回归。

时延-时间曲线的样本收集粒度为RTP包级。对于时间轴坐标，零点设为整个媒体传输过程，第一个RTP包的到达时间。对于累计时延轴，每个RTP包样本点使用所在RTP包组的累计时延作为纵轴坐标。线性回归以固定长度的滑动窗口为尺度进行计算，长度单位是收到的RTP包个数，现有代码中设置为20。






拓展卡尔曼滤波


基于丢包的码率控制

基于单向队列时延梯度的码率控制机制，在

\begin{enumerate}
    \item 链路中队列缓存过小时，过载探测器将无法探测单向队列时延梯度；
    \item 与其他基于丢包的拥塞控制算法共存，WebRTC流会陷入饥饿；
\end{enumerate}

两种情况下将失灵。由此引入基于丢包的码率控制模块，作为兜底。

由草案\parencite{rmcat-gcc}规定，位于发送端的丢包控制器依据草案\parencite{twcc}约定的transport-wide feedback message报文得到接收方收到的sequence number，自行计算丢包率。该报文一般每个视频帧回传一次（每30ms-100ms回传一次）；或依据草案\parencite{rmcat-remb}，通过接收方回传的RTCP Receiver Report报文，得到丢包率。

设丢包率为$Loss$,估计发送码率为$A_s$,控制间隔为$i$。有以下算法：

\begin{equation*}
A_s(i) = \begin{cases}
    A_s(i-1) * (1- 0.5*Loss),& Loss > 0.1\\
    1.05 * A_s(i-1),& Loss < 0.02\\
    A_s(i-1), & \text{otherwise}
\end{cases}
\end{equation*}

在丢包率低于$2\%$时，认为是与流自身拥塞无关的偶然丢包，忽略并乘性增加估计码率。如果拥塞丢包存在，那么丢包率会很快超过$10\%$,此时减少估计码率。







